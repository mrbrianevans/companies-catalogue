name: Data Jobs
on:
  schedule:
    - cron: '0 6 * * *'
  workflow_dispatch:

env:
  SINK_BUCKET: ${{ secrets.SINK_BUCKET || vars.SINK_BUCKET }}
  S3_ENDPOINT: ${{ secrets.S3_ENDPOINT || vars.S3_ENDPOINT }}
  S3_ACCESS_KEY_ID: ${{ secrets.S3_ACCESS_KEY_ID }}
  S3_SECRET_ACCESS_KEY: ${{ secrets.S3_SECRET_ACCESS_KEY }}
  S3_REGION: ${{ secrets.S3_REGION || vars.S3_REGION }}
  STREAM_KEY: ${{ secrets.STREAM_KEY }}

jobs:
  capture-streams:
    name: Capture event streams
    runs-on: ubuntu-latest

    defaults:
      run:
        working-directory: eventCapture

    steps:
      - name: Checkout repository
        uses: actions/checkout@v6

      - name: Use Bun
        uses: oven-sh/setup-bun@v2

      - name: Install dependencies
        run: bun install --frozen-lockfile

      - run: bun captureStream.ts charges
      - run: bun captureStream.ts officers
      - run: bun captureStream.ts persons-with-significant-control
      - run: bun captureStream.ts persons-with-significant-control-statements
      - run: bun captureStream.ts filings
      - run: bun captureStream.ts companies
      - run: bun captureStream.ts insolvency-cases
      - run: bun captureStream.ts company-exemptions
      - run: bun captureStream.ts disqualified-officers


  # TODO: new job in historicalEvents workdir to load lake house. depends on capture job
